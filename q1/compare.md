# ðŸ” Tokenization Analysis Report
 
 ## âœ‰ï¸ Input Sentence:
 > "The cat sat on the mat because it was tired."
 
 ---
 
 ## 1. *Byte Pair Encoding (BPE â€“ GPT-2 Style)*
 
 - *Tokens*:  
   ['The', 'Ä cat', 'Ä sat', 'Ä on', 'Ä the', 'Ä mat', 'Ä because', 'Ä it', 'Ä was', 'Ä tired', '.']  
 - *Token IDs*:  
   [464, 3797, 3332, 319, 262, 2603, 780, 340, 373, 10032, 13]  
 - *Token Count*: 11
 
 ---
 
 ## 2. *WordPiece (Used by BERT Models)*
 
 - *Tokens*:  
   ['the', 'cat', 'sat', 'on', 'the', 'mat', 'because', 'it', 'was', 'tired', '.']  
 - *Token IDs*:  
   [1996, 4937, 2938, 2006, 1996, 13523, 2138, 2009, 2001, 5458, 1012]  
 - *Token Count*: 11
 
 ---
 
 ## 3. *SentencePiece (T5 Tokenizer)*
 
 - *Tokens*:  
   ['â–The', 'â–cat', 'â–', 's', 'at', 'â–on', 'â–the', 'â–mat', 'â–because', 'â–it', 'â–was', 'â–tired', '.']  
 - *Token IDs*:  
   [37, 1712, 3, 7, 144, 30, 8, 6928, 250, 34, 47, 7718, 5]  
 - *Token Count*: 13
 
 ---
 
 ## âš™ï¸ Why the Tokenizations Vary
 
 Each tokenization method follows its own rules:
 
 - *BPE (Byte Pair Encoding)*: Builds words by merging common subword units (e.g., "ti" + "red" â†’ "tired").
 - *WordPiece*: Uses prefix markers (##) to denote subwords that continue a word.
 - *SentencePiece*: Treats whitespace as a token (â–), allowing more flexible segmentation.
 
 Tokenization differences arise due to the tokenizer's internal vocabulary and segmentation strategy.
 
 ---
 
 ## ðŸ§  BERT-Style Masked Word Predictions
 
 ### ðŸ”¸ Original Prompt:
 > "The cat sat on the *[MASK]* because it was *[MASK]*."
 
 ### ðŸ”¹ First Mask Prediction ([MASK] â†’ "the _")
 - *floor* (probability: 0.2534)  
 - *couch* (0.0875)  
 - *bed* (0.0874)
 
 ### ðŸ”¹ Second Mask Prediction (was ___)
 - *cold* (0.0537)  
 - *hungry* (0.0453)  
 - *warm* (0.0316)
 
 ---
 
 ## ðŸ“Œ Interpretation
 
 The BERT model demonstrates contextual understanding:
 
 - For "sat on the ___", it suggests common resting places: *floor, **bed, **couch*.
 - For "because it was ___", it selects logical states: *cold, **hungry, **warm*.
 
 These predictions reflect real-world associations learned from large-scale text datasets such as Wikipedia and books.